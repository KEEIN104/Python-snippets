{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url-encode-urllib.py2\n",
    "import urllib\n",
    "\n",
    "query_args = {'q':'query string','foo':'bar'}\n",
    "encoded_args = urllib.urlencode(query_args)\n",
    "\n",
    "print 'Encoded:',encoded_args\n",
    "\n",
    "url = 'http://localhost:8080/?'+encoded_args\n",
    "print urllib.urlopen(url).read()\n",
    "\n",
    "#\n",
    "print '------------------'\n",
    "\n",
    "query_args = {'foo':['foo1','foo2']}\n",
    "print 'Single\t:',urllib.urlencode(query_args)\n",
    "print 'Sequence:',urllib.urlencode(query_args, doseq=True)\n",
    "\n",
    "#\n",
    "print '-------------------'\n",
    "\n",
    "url = 'http://localhost:8080/~kishor/'\n",
    "print 'urlencode()\t:',urllib.urlencode({'url':url})\n",
    "print 'quote()\t\t:',urllib.quote(url)\n",
    "print 'quote_plus()\t:',urllib.quote_plus(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url-retrieve-urllib.py2\n",
    "\n",
    "#urllib first encounter\n",
    "'''\n",
    "access remote resources that do not need authentication, cookies etc.\n",
    "'''\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def reporthook(blocks_read, block_size, total_size):\n",
    "\t'''\n",
    "\ttotal_size -is reported in bytes.\n",
    "\tblock_size -is the amount read each time.\n",
    "\tblocks_read -is the number of blocks successfully read.\n",
    "\t'''\n",
    "\tif not blocks_read:\n",
    "\t\tprint 'Connection opened...'\n",
    "\t\treturn\n",
    "\tif total_size < 0:\n",
    "\t\t#Unknown size\n",
    "\t\tprint 'Read %d blocks (%d bytes)'%(blocks_read, blocks_read*block_size)\n",
    "\telse:\n",
    "\t\tamount_read = blocks_read * block_size\n",
    "\t\tprint 'Read %d blocks, or &d/%d' %(blocks_read, amount_read, total_size)\n",
    "\treturn\n",
    "\n",
    "try:\n",
    "\tfilename, msg = urllib.urlretrieve('http://www.google.com/',reporthook=reporthook)\n",
    "\tprint\n",
    "\tprint 'File\t----', filename\n",
    "\tprint 'Headers----\t:'\n",
    "\tprint msg\n",
    "\tprint 'File exists before cleanup--', os.path.exists(filename) \n",
    "\n",
    "finally:\n",
    "\turllib.urlcleanup()\n",
    "\n",
    "print 'File still exists--', os.path.exists(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url-vs-path-urllib.py2\n",
    "import os\n",
    "\n",
    "from urllib import pathname2url, url2pathname\n",
    "\n",
    "print '==Default=='\n",
    "path = '/a/b/c'\n",
    "print 'Original:',path\n",
    "print 'URL\t:',pathname2url(path)\n",
    "print 'Path\t:',url2pathname('/d/e/f')\n",
    "print\n",
    "\n",
    "'''\n",
    "more info:\n",
    ">urllib (http://docs.python.org/lib/module-urllib.html)\n",
    ">urllib2 (pg 657)\n",
    ">urlparse (pg 638) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url210.py\n",
    "#other encodings\n",
    "#base32, base16(hex)\n",
    "import base64\n",
    "original_string = 'This is the data, in the clear.'\n",
    "print 'Original:',original_string\n",
    "\n",
    "print '----------base32----------'\n",
    "encoded_string = base64.b32encode(original_string)\n",
    "print 'Encoded:',encoded_string\n",
    "\n",
    "decoded_string = base64.b32decode(encoded_string)\n",
    "print 'Decoded:',decoded_string\n",
    "\n",
    "print\n",
    "print '------hex-base16----------'\n",
    "encoded_string = base64.b16encode(original_string)\n",
    "print 'Encoded:',encoded_string\n",
    "\n",
    "decoded_string = base64.b16decode(encoded_string)\n",
    "print 'Decoded:',decoded_string\n",
    "\n",
    "'''\n",
    "more info:\n",
    ">base64 (http://docs.python.org/library/base64.html)\n",
    ">RFC 3548 (http://tools.ietf.org/html/rfc3548.html) :base 64, base 32, base 16 data encodings\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-2.py2\n",
    "\n",
    "#encoding arguments and appending them in url\n",
    "import urllib\n",
    "import urllib2\n",
    "\n",
    "query_args = {'q':'query string','foo':'bar'}\n",
    "encoded_args = urllib.urlencode(query_args)\n",
    "print 'Encoded:',encoded_args\n",
    "\n",
    "url = 'http://localhost:8080/?'+encoded_args\n",
    "print urllib2.urlopen(url).read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-3.py2\n",
    "import urllib\n",
    "import urllib2\n",
    "\n",
    "query_args = {'q':'query string','foo':'bar'}\n",
    "encoded_args = urllib.urlencode(query_args)\n",
    "url = 'http://localhost:8080/'\n",
    "print urllib2.urlopen(url, encoded_args).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-4.py2\n",
    "#adding outgoing headers\n",
    "import urllib2\n",
    "\n",
    "request = urllib2.Request('http://localhost:8080/')\n",
    "request.add_header(\n",
    "\t\t'User-agent',\n",
    "\t\t'PyMOTW (http://www.doughellmann.com/PyMOTW/)',\n",
    "\t\t)\n",
    "\n",
    "response = urllib2.urlopen(request)\n",
    "data = response.read()\n",
    "print data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ulr2-5.py2\n",
    "import urllib\n",
    "import urllib2\n",
    "\n",
    "query_args = {'q':'query string','foo':'bar'}\n",
    "\n",
    "request = urllib2.Request('http://localhost:8080/')\n",
    "print 'Request method before data:',request.get_method()\n",
    "\n",
    "request.add_data(urllib.urlencode(query_args))\n",
    "print 'Request method after data:',request.get_method()\n",
    "request.add_header(\n",
    "\t\t'user-agent',\n",
    "\t\t'PyMOTW (http://www.douthellmann.com/PyMOTW/)',\n",
    "\t\t)\n",
    "\n",
    "print\n",
    "print 'OUTGOING DATA:'\n",
    "print request.get_data()\n",
    "\n",
    "print\n",
    "print 'SERVER RESPONSE:'\n",
    "print urllib2.urlopen(request).read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-6-fileuoload.py2\n",
    "#uploading files\n",
    "import itertools\n",
    "import mimetools\n",
    "import mimetypes\n",
    "from cStringIO import StringIO\n",
    "import urllib\n",
    "import urllib2\n",
    "\n",
    "class MultiPartForm(object):\n",
    "\t\"\"\"Accumulate the data to be used when posting a form\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.form_fields = []\n",
    "\t\tself.files = []\n",
    "\t\tself.boundary = mimetools.choose_boundary()\n",
    "\t\treturn\n",
    "\t\n",
    "\tdef get_content_type(self):\n",
    "\t\treturn 'multipart/form-data; boundary=%s'%self.boundary\n",
    "\n",
    "\tdef add_field(self,name,value):\n",
    "\t\t'''Add a simple field to the form data.'''\n",
    "\t\tself.form_fields.append((name,value))\n",
    "\t\treturn\n",
    "\n",
    "\tdef add_file(self,fieldname,filename,fileHandle,mimetype=None):\n",
    "\t\t'''Add a file to be uploaded'''\n",
    "\t\tbody = fileHandle.read()\n",
    "\t\tif mimetype is None:\n",
    "\t\t\tmimetype = (mimetypes.guess_type(filename)[0] \n",
    "\t\t\t\t\t\tor\n",
    "\t\t\t\t\t\t'application/octet-stream'\n",
    "\t\t\t\t\t\t)\n",
    "\t\tself.files.append((fieldname,filename,mimetype,body))\n",
    "\t\treturn\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\t'''Return a string representing the form data, including attached files'''\n",
    "\t\t#Build a list of lists, each containing \"lines\" of the\n",
    "\t\t#request. Each part os separated by a boundary string. \n",
    "\t\t#once the list is built, return the string wherre each\n",
    "\t\t#line is separated by '\\r\\n'.\n",
    "\t\tparts = []\n",
    "\t\tpart_boundary = '--'+self.boundary\n",
    "\t\t\n",
    "\t\t#add the form fields\n",
    "\t\tparts.extend(\n",
    "\t\t\t[part_boundary,\n",
    "\t\t\t 'Content-Disposition: form-data; name=\"%s\"' %name,\n",
    "\t\t\t '',\n",
    "\t\t\t value,\n",
    "\t\t\t]\n",
    "\t\t\tfor name, value in self.form_fields\n",
    "\t\t\t)\n",
    "\t\n",
    "\t\t#add files to upload\n",
    "\t\tparts.extend([\n",
    "\t\t\tpart_boundary,\n",
    "\t\t\t'Content-Disposition:file; name=\"%s\"; filename=\"%s\"'% \\\n",
    "\t\t\t\t(field_name,filename),\n",
    "\t\t\t'Content-type:%s'%content_type,\n",
    "\t\t\t'',\n",
    "\t\t\tbody,\n",
    "\t\t\t]\n",
    "\t\t\tfor field_name, filename, content_type, body in self.files\n",
    "\t\t\t)\n",
    "\n",
    "\t\t#Flatten the list and add closing boundary marker, and\n",
    "\t\t#then return CR+LF separated data\n",
    "\t\tflattened = list(itertools.chain(*parts))\n",
    "\t\tflattened.append('--'+self.boundary+'--')\n",
    "\t\tflattened.append('')\n",
    "\t\treturn '\\r\\n'.join(flattened)\n",
    "\n",
    "if __name__=='__main__':\n",
    "\t#Create the form with simple fields\n",
    "\tform = MultiPartForm()\n",
    "\tform.add_field('firstname','kishore')\n",
    "\tform.add_field('lastname','kumar')\n",
    "\n",
    "\t#add a fake file\n",
    "\tform.add_file(\n",
    "\t\t\t'biography','bio.txt',\n",
    "\t\t\tfileHandle=StringIO('Python developer and blogger.'))\n",
    "\n",
    "\t#Build a request\n",
    "\trequest = urllib2.Request('http://localhost:8080/')\n",
    "\trequest.add_header(\n",
    "\t\t'User-agent',\n",
    "\t\t'PyMOTW (http://www.doughellmann.com/PyMOTW/)')\n",
    "\tbody = str(form)\n",
    "\trequest.add_header('Content-Type',form.get_content_type())\n",
    "\trequest.add_header('Content-Length',len(body))\n",
    "\trequest.add_data(body)\n",
    "\n",
    "\tprint\n",
    "\tprint 'OUTGOING DATA:'\n",
    "\tprint request.get_data()\n",
    "\n",
    "\tprint\n",
    "\tprint 'SERVER RESPONSE:'\n",
    "\tprint urllib2.urlopen(request).read()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-7.py2\n",
    "\n",
    "#creating cusom protocol handlers\n",
    "#nfs handler\n",
    "\n",
    "import mimetypes\n",
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "import urllib2\n",
    "\n",
    "class NFSFile(file):\n",
    "\tdef __init__(self,tempdir,filename):\n",
    "\t\tself.tempdir = tempdir\n",
    "\t\tfile.__init__(self,filename,'rb')\n",
    "\tdef close(self):\n",
    "\t\tprint 'NFSFile:'\n",
    "\t\tprint '\tunmounting %s'%os.path.basename(self.tempdir)\n",
    "\t\tprint '\twhen %s is closed'%os.path.basename(self.name)\n",
    "\t\treturn file.close(self)\n",
    "\n",
    "class FauxNFSHandler(urllib2.BaseHandler):\n",
    "\t\n",
    "\tdef __init__(self,tempdir):\n",
    "\t\tself.tempdir = tempdir\n",
    "\n",
    "\tdef nfs_open(self,req):\n",
    "\t\turl = req.get_selector()\n",
    "\t\tdirectory_name, file_name = os.path.split(url)\n",
    "\t\tserver_name = req.get_host()\n",
    "\t\tprint 'FauxNFSHandler simulating mount:'\n",
    "\t\tprint '\tRemote path: %s'%directory_name\n",
    "\t\tprint '\tServer\t: %s'%server_name\n",
    "\t\tprint '\tLocal path\t: %s'%os.path.basename(tempdir)\n",
    "\t\tprint '\tFilename: %s'%file_name\n",
    "\t\tlocal_file = os.path.join(tempdir,file_name)\n",
    "\t\tfp = NFSFile(tempdir, local_file)\n",
    "\t\tcontent_type = (mimetypes.guess_type(file_name)[0]\n",
    "\t\t\tor\n",
    "\t\t\t'application/octet-stream'\n",
    "\t\t\t)\n",
    "\t\tstats = os.stat(local_file)\n",
    "\t\tsize = stats.st_size\n",
    "\t\theaders = {'Content-type':content_type,\n",
    "\t\t\t\t\t  'Content-length':size,\n",
    "\t\t\t\t\t}\n",
    "\t\treturn urllib.addinfourl(fp,headers,req.get_full_url())\n",
    "\n",
    "if __name__=='__main__':\n",
    "\ttempdir = tempfile.mkdtemp()\n",
    "\ttry:\n",
    "\t\t#populate the temporary file for the simulation\n",
    "\t\twith open(os.path.join(tempdir,'file.txt'),'wt') as f:\n",
    "\t\t\tf.write('Contents of file.txt')\n",
    "\n",
    "\t\t#construc an opener with our NFS handler\n",
    "\t\t#and register it as the default opener\n",
    "\t\topener = urllib2.build_opener(FauxNFSHandler(tempdir))\n",
    "\t\turllib2.install_opener(opener)\n",
    "\n",
    "\t\t#open the file through a url\n",
    "\t\tresponse = urllib2.urlopen(\n",
    "\t\t\t'nfs://remote_server/path/to/the/file.txt'\n",
    "\t\t\t)\n",
    "\n",
    "\t\tprint\n",
    "\t\tprint 'READ CONTENTS:',response.read()\n",
    "\t\tprint 'URL\t:',response.geturl()\n",
    "\t\tprint 'HEADERS:'\n",
    "\t\tfor name, value in sorted(response.info().items):\n",
    "\t\t\tprint ' %-15s = %s' %(name, value)\n",
    "\t\tresponse.close()\n",
    "\tfinally:\n",
    "\t\tos.remove(os.path.join(tempdir,'file.txt'))\n",
    "\t\tos.removedirs(tempdir)\t\n",
    "\n",
    "'''\n",
    "more info:\n",
    ">urllib2 (http://docs.pyhton.org/library/urllib2.html\n",
    ">urllib (page 651)\n",
    ">urlparse (page 638)\n",
    ">urllib2 - the missing manual (www.voidspace.org.uk/python/articles/urllib2.shtml)\n",
    ">upload scripts (www.voidspace.org.uk/python/cgi.shtml#upload)\n",
    ">HTTP client to POST using multipart/form data (http://code.activestate.com/recipes/146306) cookbook: how to encode and post data, including files over HTTP\n",
    ">Form content types (www.w3.org/TR/REC-html40/interact/forms.thrl#h-17.13.4)\n",
    ">mimetypes :map filenames to mimetype\n",
    ">mimetools :tools for parsing MIME messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-8-base64.py2\n",
    "\n",
    "#base64 encode/decode\n",
    "import base64\n",
    "import textwrap\n",
    "\n",
    "#Loadthis source file and strip the header.\n",
    "with open(__file__,'rt') as input:\n",
    "\traw = input.read()\n",
    "\tinitial_data = raw.split('#end_pymotw_header')[1]\n",
    "\n",
    "encoded_data = base64.b64encode(initial_data)\n",
    "\n",
    "num_initial = len(initial_data)\n",
    "\n",
    "#There will never be more than two padding bytes\n",
    "padding = 3-(num_initial%3)\n",
    "\n",
    "print '%d bytes before encoding' %num_initial\n",
    "print 'Except %d padding bytes' %padding\n",
    "print '%d bytes after encoding' % len(encoded_data)\n",
    "print\n",
    "print encoded_data\n",
    "\n",
    "#======decode===========\n",
    "print '----------------------------'\n",
    "original_string = 'This is the data, in the clear.'\n",
    "print 'original :',original_string\n",
    "\n",
    "encoded_string =base64.b64encode(original_string)\n",
    "print 'Encoded :', encoded_string\n",
    "\n",
    "decoded_string = base64.b64decode(encoded_string)\n",
    "print 'Decoded :',decoded_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-9.py2\n",
    "#url safe encoding\n",
    "#replace + with a -and / with a underscore _.all else same \n",
    "import base64\n",
    "\n",
    "encode_with_pluses = chr(251) + chr(239)\n",
    "encode_with_slashes = chr(255) * 2\n",
    "\n",
    "for original in [encode_with_pluses,encode_with_slashes]:\n",
    "\tprint 'Original\t:',repr(original)\n",
    "\tprint 'Standard Encoding:',base64.standard_b64encode(original)\n",
    "\tprint 'URL-safe encoding:',base64.urlsafe_b64encode(original)\n",
    "\tprint \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2-urllib2-1.py2\n",
    "import urllib2\n",
    "\n",
    "response = urllib2.urlopen('http://localhost:8080/')\n",
    "print 'RESPONSE:',response\n",
    "print 'URL \t:',response.geturl()\n",
    "\n",
    "headers = response.info()\n",
    "print 'DATE\t:',headers['date']\n",
    "print 'HEADERS\t:'\n",
    "print '--------------'\n",
    "print headers\n",
    "\n",
    "data = response.read()\n",
    "print 'LENGTH:',len(data)\n",
    "print 'DATA\t:'\n",
    "print '------------------'\n",
    "print data\n",
    "\n",
    "\n",
    "#---------v2---\n",
    "print '======================'\n",
    "#import urllib2\n",
    "\n",
    "response = urllib2.urlopen('http://localhost:8080/')\n",
    "for line in response:\n",
    "\tprint line.rstrip()\n",
    "#=================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlfile.txt\n",
    "1\n",
    "2\n",
    "http://www.google.com\n",
    "http://www.bing.com#\n",
    "http://www.youtube.com#\n",
    "http://www.facebook.com#\n",
    "http://www.gmail.com#\n",
    "http://www.outlook.com#\n",
    "https://github.com/manageyp/manageyp.github.com/blob/master/attachments/pdfs/The Python Standard Library by Example.pdf#\n",
    "https://github.com#\n",
    "http://www.amazon.in#\n",
    "http://www.unicode.org/charts/#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlLib.py\n",
    "#\n",
    "#----------------URL METADATA -----------------------\n",
    "#class url(object)\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "'''\n",
    "urllib.request\n",
    "    urllib.request.urlopen(url...)\n",
    "        geturl()\n",
    "        info()\n",
    "        getcode()\n",
    "'''\n",
    "\n",
    "urlPath='http://www.google.com'\n",
    "\n",
    "def urlfile():\n",
    "#writes user input urlPath in urlfile.txt............. file filecreation\n",
    "    while True:\n",
    "        print('urlPath: ',end=' ')\n",
    "        urlPath = input()#'http://www.google.com'\n",
    "        if urlPath =='':\n",
    "            break\n",
    "        #append infile\n",
    "        fileobject = open('/home/kishore/p/urlfile.txt', 'a+')\n",
    "        fileobject.write(\"http://www.%s\\n\"%urlPath)\n",
    "        fileobject.close()    \n",
    "    return urlPath\n",
    "    #RETURN urlPath string (last entry)\n",
    "\n",
    "def makecapsule(urlPath):\n",
    "#url capsule from single url----------metadata in url\n",
    "    x=[]\n",
    "    x = x + [urllib.request.urlopen(urlPath)]\n",
    "    xhead = x[0]\n",
    "    x = x + [xhead.geturl(), xhead.info(), xhead.getcode()]\n",
    "    return x\n",
    "    #RETURN A LIST [x]\n",
    "\n",
    "def readurlfile(file):\n",
    "    \n",
    "    urls = []\n",
    "    fileobject = open(file,'r')\n",
    "    data = fileobject.read()\n",
    "    data = data.split()\n",
    "    for element in data:\n",
    "        if 'http' in element and '#' not in element:\n",
    "            urls = urls + [element]\n",
    "    fileobject.close()\n",
    "    return urls\n",
    "    #RETURN [...]\n",
    "\n",
    "#............................................... DATA PROCESSING................            \n",
    "#------------------makedata(...)\n",
    "def makedata(urlist):\n",
    "    data = []\n",
    "    for url in urlist:\n",
    "        data = data + makecapsule(url)\n",
    "    datalist = data\n",
    "    return datalist\n",
    "\n",
    "\n",
    "#DISPLAY\n",
    "'''................................................CAPSULE DISPLAY.......................\n",
    "#..........................typedatadisplay\n",
    "def fn1(capsule):\n",
    "    count = 0\n",
    "    for element in capsule:\n",
    "        if count == 0:\n",
    "            print(type(element))\n",
    "            count+=1\n",
    "        else:\n",
    "            print('\\t'+str(type(element)))\n",
    "\n",
    "#..........................datadisplay\n",
    "def fn2(capsule):\n",
    "    count = 0\n",
    "    for element in capsule:\n",
    "        if count == 0:\n",
    "            print(element)\n",
    "            count+=1\n",
    "        else:\n",
    "            print('\\t'+str(element))\n",
    "'''\n",
    "'''x[0]= urllib.request.urlopen(urlPath)\n",
    "x[1] = x.geturl()\n",
    "x[2] = x.info()\n",
    "x[3] = x.getcode()\n",
    "'''\n",
    "'''\n",
    "x = urllib.request.urlopen(urlPath)\n",
    "print(type(x))\n",
    "print(type(x.geturl()))\n",
    "''''''typx.info()\n",
    "x[3] = x.getcode()\n",
    "\n",
    "print(x[0])\n",
    "for i in range(1,4):\n",
    "    print('\\t',end = ' ')\n",
    "    print(x[i])\n",
    "'''\n",
    "#-------datadisplay-----------fn3\n",
    "def printdatafn3(datalist):\n",
    "    #print(datalist)\n",
    "    count = 0\n",
    "    for listin in datalist:\n",
    "        count +=1\n",
    "        print('----------------------------------------------------------------------%d------'%count)\n",
    "        print(listin)\n",
    "        \n",
    "\n",
    "\n",
    "##########################PROGRAM\n",
    "printdatafn3(makedata(readurlfile('/home/kishore/p/urlfile.txt')))\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlparser.py2\n",
    "#pyhton2.7\n",
    "#urlparser.py\n",
    "\n",
    "from urlparse import urlparse\n",
    "\n",
    "FIXEDURL = 'http://www.google.com'\n",
    "URL1 = 'http://netloc/path;param?query=arg#frag'\n",
    "URL2 = 'http://user:pwd@NetLoc:80/path;param?query=arg#frag'\n",
    "\n",
    "def urlparser(url):\n",
    "    parsed = urlparse(url)\n",
    "    print '----------------------------------------'\n",
    "    print 'scheme: ', parsed.scheme \n",
    "    print 'netloc: ', parsed.netloc\n",
    "    print 'path: ',parsed.path\n",
    "    print 'params: ',parsed.params\n",
    "    print 'query: ',parsed.query\n",
    "    print 'fragment: ',parsed.fragment\n",
    "    print 'username: ',parsed.username\n",
    "    print 'password: ',parsed.password\n",
    "    print 'hostname',parsed.hostname, '(netloc in lowercase)'\n",
    "    print 'port: ',parsed.port\n",
    "    print '\\n'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "up0 = urlparser(FIXEDURL)\n",
    "up1 = urlparser(URL1)\n",
    "up2 = urlparser(URL2)\n",
    "print up1\n",
    "print up2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www1.py2\n",
    "#The worlds simpliest web browser\n",
    "\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('www.py4inf.com',80))\n",
    "\n",
    "mysock.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\\n\\n')\n",
    "\n",
    "while True:\n",
    "\tdata = mysock.recv(512)\n",
    "\tif (len(data)<1):\n",
    "\t\tbreak\n",
    "\tprint data\n",
    "\n",
    "mysock.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www2.py2\n",
    "#retrieving an image over HTTP\n",
    "\n",
    "import socket\n",
    "import time\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('www.py4inf.com',80))\n",
    "mysock.send('GET http://www.py4inf.com/cover.jpg HTTP/1.0\\n\\n')\n",
    "\n",
    "count = 0\n",
    "picture=\"\";\n",
    "while True:\n",
    "\tdata = mysock.recv(5120)\n",
    "\tif (len(data)<1):break\n",
    "\t#time.sleep(0.25)\t#delay helps buffering\n",
    "\tcount = count +len(data)\n",
    "\tprint len(data), count\n",
    "\tpicture = picture+data\n",
    "\n",
    "mysock.close()\n",
    "\n",
    "#Lookfor the end of header (2 CRLF)\n",
    "pos = picture.find(\"\\r\\n\\r\\n\");\n",
    "print 'Header length',pos\n",
    "print picture[:pos]\n",
    "\n",
    "#skip past the header and save the picture data\n",
    "picture = picture[pos+4:]\n",
    "fhand = open(\"stuff.jpg\",\"wb\")\n",
    "fhand.write(picture);\n",
    "fhand.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www3.py2\n",
    "#\n",
    "#parsing html using regular expressions\n",
    "# return all links in entered web page\n",
    "\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "url = raw_input('Enter url-')\n",
    "html = urllib.urlopen(url).read()\n",
    "links = re.findall('href=\"(http://.*?)\"',html)\n",
    "for link in links:\n",
    "\tprint link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www4.py2\n",
    "import urllib\n",
    "from BeautifulSoup import *\n",
    "\n",
    "url = raw_input('Enter url - ')\n",
    "html = urllib.urlopen(url).read()\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "#href : hypertext reference\n",
    "#Retrieve all of the anchor tags\n",
    "tags =soup('a')\n",
    "for tag in tags:\n",
    "\tprint tag.get('href',None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www5\n",
    "#import urllib\n",
    "from BeautifulSoup import *\n",
    "\n",
    "url = raw_input('Enter url - ')\n",
    "html = urllib.urlopen(url).read()\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "#Retrieve all of the anchor tags\n",
    "tags =soup('a')\n",
    "for tag in tags:\n",
    "\tprint tag.get('href',None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www5.py2\n",
    "\n",
    "\n",
    "#parsing html using beautifulsoup\n",
    "\n",
    "import urllib\n",
    "from BeautifulSoup import *\n",
    "\n",
    "url = raw_input('Enter url - ')\n",
    "html = urllib.urlopen(url).read()\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "#href : hypertext reference\n",
    "#Retrieve all of the anchor tags\n",
    "tags =soup('a')\n",
    "for tag in tags:\n",
    "\tprint 'TAG:',tag\t\n",
    "\tprint 'URL:',tag.get('href',None)\n",
    "\tprint 'Content:',tag.contents[0]\n",
    "\tprint 'Attrs:',tag.attrs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www6.py2\n",
    "import urllib\n",
    "\n",
    "img = urllib.urlopen('http://www.py4inf.com/cover.jpg')\n",
    "fhand = open('cover.jpg','w')\n",
    "size = 0\n",
    "while True:\n",
    "\t#downloading data in chunks prevents memory overload\n",
    "\tinfo = img.read(100000)\t#100000 characters at a time\n",
    "\tif len(info)<1: break\n",
    "\tsize = size+len(info)\n",
    "\tfhand.write(info)\n",
    "\n",
    "print size,'characters copied.'\n",
    "fhand.close()\n",
    "\n",
    "'''\n",
    "port:\n",
    "web traffic generally uses -port 80; email traffic -port 25\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www7simplebrowser.py\n",
    "\n",
    "#simpliest web browser inpython an Tkinter\n",
    "from tkinter import *\n",
    "import urllib.request\n",
    "def go():\n",
    "\ttext.delete(1.0, END)\n",
    "\twith urllib.request.urlopen(entry.get()) as response:\n",
    "\t\treceived_html = response.read()\n",
    "\ttext.insert(1.0, received_html)\n",
    "\n",
    "browser_window = Tk()\n",
    "browser_window.title('custom browser')\n",
    "label = Label(browser_window, text= 'Enter URL:')\n",
    "entry = Entry(browser_window)\n",
    "entry.insert(END, \"http://knowpapa.com\")\n",
    "button = Button(browser_window, text='Go', command = go)\n",
    "text = Text(browser_window)\n",
    "label.pack(side=TOP)\n",
    "entry.pack(side=TOP)\n",
    "button.pack(side=TOP)\n",
    "text.pack(side= TOP)\n",
    "browser_window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webscrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codetest.py\n",
    "import sys\n",
    "sys.path.append('/home/kishore/p/webscrap/')\n",
    "\n",
    "from webscrapfunc import *\n",
    "\n",
    "url0='http://example.webscraping.com'\n",
    "x = 'example.webscraping.com/(index|view)/'\n",
    "#link_crawler(url,x)\n",
    "\n",
    "uurl =raw_input('url: ')\n",
    "uuurl = 'http://www.'+str(uurl)\n",
    "\n",
    "y = download(uuurl)\n",
    "print y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function1.py\n",
    "#! python\n",
    "\n",
    "#web scraping function1\n",
    "#download_function.py2\n",
    "\n",
    "import urllib2\n",
    "\n",
    "def download(url, user_agent = 'orek', num_retries=2):\n",
    "\tprint 'Downloading:',url\n",
    "\theaders = {'User_agent':user_agent}\n",
    "\trequest = urllib2.Request(url,headers = headers)\n",
    "   \n",
    "\ttry:\n",
    "      \t\thtml = urllib2.urlopen(url).read()\n",
    "\t\n",
    "   \texcept urllib2.URLError as e:\n",
    "      \t\tprint 'Download error:',e.reason\n",
    "     \t\thtml = None\n",
    "\t\t\n",
    "\t      \tif num_retries > 0:\n",
    "        \t\tif hasattr(e,'code') and 500 <= e.code < 600:\n",
    "            \t\t\t#recursively retry 5xx http errors\n",
    "            \t\t\treturn download(url, num_retries-1)\n",
    "\treturn html\n",
    "\n",
    "#test run\n",
    "#download('http://httpstat.us/500')\n",
    "\n",
    "#! python\n",
    "\n",
    "#download_function.py2\n",
    "import urllib2\n",
    "\n",
    "def download(url, user_agent = 'orek', num_retries=2):\n",
    "   print 'Downloading:',url\n",
    "   headers = {'User_agent':user_agent}\n",
    "   request = urllib2.Request(url,headers = headers)\n",
    "   \n",
    "   try:\n",
    "      html = urllib2.urlopen(request).read()\n",
    "\t\n",
    "   except urllib2.URLError as e:\n",
    "      print 'Download error:', e.reason\n",
    "      html = None\n",
    "\t\t\n",
    "      if num_retries > 0:\n",
    "         if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "            #recursively retry 5xx http errors\n",
    "            return download(url, num_retries-1)\n",
    "   return html\n",
    "\n",
    "#test run\n",
    "#download('http://httpstat.us/500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function2.py2\n",
    "\n",
    "#web scraping function\n",
    "#crawl_sitemap.py\n",
    "\n",
    "from webscrapfunc import *\n",
    "\n",
    "def crawl_sitemap(url):\n",
    "\t#download the sitemap file\n",
    "\tsitemap = download(url)\n",
    "\t#extract the sitemap links\n",
    "\tlinks =  re.findall('<loc>(.*?)</loc>', sitemap)\n",
    "\t#download each link\n",
    "\tfor link in links:\n",
    "\t\thtml = download(link)\n",
    "\t\t# scrape html here\n",
    "\t\t#...\n",
    "\n",
    "        \n",
    "#copy\n",
    "#crawl_sitemap.py2\n",
    "\n",
    "def crawl_sitemap(url):\n",
    "   import re\n",
    "\t#download the sitemap file\n",
    "   sitemap = download(url)\n",
    "\t#extract the sitemap links\n",
    "   links =  re.findall('<loc>(.*?)</loc>', sitemap)\n",
    "\t#download each link\n",
    "   for link in links:\n",
    "\t\thtml = download(link)\n",
    "\t\t# scrape html here\n",
    "\t\t#...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web scraping function\n",
    "#function3.py\n",
    "\n",
    "from webscrapfunc import *\n",
    "\n",
    "#url = 'http://example.webscraping.com/view-%d'%page\n",
    "\n",
    "def id_iter_crawl(url):\n",
    "\timport itertools\n",
    "\tmax_errors = 5\n",
    "\tnum_errors = 0\n",
    "\tfor page in itertools.count(1):\n",
    "\t\thtml = download(url)\n",
    "\t\tif html is None:\n",
    "\t\t\t#received an error \n",
    "\t\t\tnum_errors+=1\n",
    "\t\t\tif num_errors == max_error:\n",
    "\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\t#success - can scrape the result\n",
    "\t\t\t#...\n",
    "\t\t\tnum_errors = 0\n",
    "\n",
    "            #function3.py\n",
    "\n",
    "import itertools\n",
    "max_errors = 5\n",
    "num_errors = 0\n",
    "for page in itertools.count(1):\n",
    "\turl = 'http://example.webscraping.com/view-%d'%page\n",
    "\thtml = download(url)\n",
    "\tif html is None:\n",
    "\t\t#received an error \n",
    "\t\tnum_errors+=1\n",
    "\t\tif num_errors == max_error:\n",
    "\t\t\tbreak\n",
    "\telse:\n",
    "\t\t#success - can scrape the result\n",
    "\t\t#...\n",
    "\t\tnum_errors = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web scraping function\n",
    "#link_crawler.py2\n",
    "#function4\n",
    "\n",
    "import re\n",
    "from webscrapfunc import *\n",
    "\n",
    "import urlparse\n",
    "def link_crawler(seed_url, link_regex):\n",
    "\t#crawl a seed_url following links matched by link_regex\n",
    "\tcrawl_queue = [seed_url]\n",
    "\t#keep track which url's have seen before\n",
    "\tseen = set(crawl_queue)\n",
    "\twhile crawl_queue:\n",
    "\t\turl = crawl_queue.pop()\n",
    "\t\thtml = download(url)\n",
    "\t\t#filter for links matching\n",
    "\t\tfor link in get_links(html):\n",
    "\t\t\tif re.match(link_regex,link):\n",
    "\t\t\t\t#form absolute link\n",
    "\t\t\t\tlink = urlparse.urljoin(seed_url,link)\t\t\t\t\n",
    "\t\t\t\t#check if have already seen this link\n",
    "\t\t\t\tif link not in seen:\n",
    "\t\t\t\t\tseen.add(link)\n",
    "\t\t\t\t\tcrawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "\t#return a list of links from html\n",
    "\t#following is a regular expression\t\n",
    "\twebpage_regex = re.compile('<a[^>+href=[\"\\'](.*?)[\"\\']',re.IGNORECASE)\n",
    "\treturn webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "#link_crawler.py2\n",
    "#function4\n",
    "\n",
    "import re\n",
    "\n",
    "def link_crawler(seed_url, link_regex):\n",
    "\t\n",
    "\tcrawl_queue = [seed_url]\n",
    "\twhile crawl_queue:\n",
    "\t\turl = crawl_queue.pop()\n",
    "\t\thtml = download(url)\n",
    "\t\t#filter for links matching\n",
    "\t\tfor link in get_links(html):\n",
    "\t\t\tif re.match(link_regex,link):\n",
    "\t\t\t\tcrawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "\twebpage_regex = re.compile('<a[^>+href=[\"\\'](.*?)[\"\\']',re.IGNORECASE)\n",
    "\treturn webpage_regex.findall(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv1.py\n",
    "import re\n",
    "import urlparse\n",
    "import urllib2\n",
    "import time\n",
    "from datetime import datetime\n",
    "import robotparser\n",
    "import Queue\n",
    "\n",
    "\n",
    "def link_crawler(seed_url, link_regex=None, delay=5, max_depth=-1, max_urls=-1, headers=None, user_agent='wswp', proxy=None, num_retries=1):\n",
    "    \"\"\"Crawl from the given seed URL following links matched by link_regex\n",
    "    \"\"\"\n",
    "    # the queue of URL's that still need to be crawled\n",
    "    crawl_queue = Queue.deque([seed_url])\n",
    "    # the URL's that have been seen and at what depth\n",
    "    seen = {seed_url: 0}\n",
    "    # track how many URL's have been downloaded\n",
    "    num_urls = 0\n",
    "    rp = get_robots(seed_url)\n",
    "    throttle = Throttle(delay)\n",
    "    headers = headers or {}\n",
    "    if user_agent:\n",
    "        headers['User-agent'] = user_agent\n",
    "\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        # check url passes robots.txt restrictions\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            throttle.wait(url)\n",
    "            html = download(url, headers, proxy=proxy, num_retries=num_retries)\n",
    "            links = []\n",
    "\n",
    "            depth = seen[url]\n",
    "            if depth != max_depth:\n",
    "                # can still crawl further\n",
    "                if link_regex:\n",
    "                    # filter for links matching our regular expression\n",
    "                    links.extend(link for link in get_links(html) if re.match(link_regex, link))\n",
    "\n",
    "                for link in links:\n",
    "                    link = normalize(seed_url, link)\n",
    "                    # check whether already crawled this link\n",
    "                    if link not in seen:\n",
    "                        seen[link] = depth + 1\n",
    "                        # check link is within same domain\n",
    "                        if same_domain(seed_url, link):\n",
    "                            # success! add this new link to queue\n",
    "                            crawl_queue.append(link)\n",
    "\n",
    "            # check whether have reached downloaded maximum\n",
    "            num_urls += 1\n",
    "            if num_urls == max_urls:\n",
    "                break\n",
    "        else:\n",
    "            print 'Blocked by robots.txt:', url\n",
    "\n",
    "\n",
    "class Throttle:\n",
    "    \"\"\"Throttle downloading by sleeping between requests to same domain\n",
    "    \"\"\"\n",
    "    def __init__(self, delay):\n",
    "        # amount of delay between downloads for each domain\n",
    "        self.delay = delay\n",
    "        # timestamp of when a domain was last accessed\n",
    "        self.domains = {}\n",
    "        \n",
    "    def wait(self, url):\n",
    "        domain = urlparse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()\n",
    "\n",
    "\n",
    "def download(url, headers, proxy, num_retries, data=None):\n",
    "    print 'Downloading:', url\n",
    "    request = urllib2.Request(url, data, headers)\n",
    "    opener = urllib2.build_opener()\n",
    "    if proxy:\n",
    "        proxy_params = {urlparse.urlparse(url).scheme: proxy}\n",
    "        opener.add_handler(urllib2.ProxyHandler(proxy_params))\n",
    "    try:\n",
    "        response = opener.open(request)\n",
    "        html = response.read()\n",
    "        code = response.code\n",
    "    except urllib2.URLError as e:\n",
    "        print 'Download error:', e.reason\n",
    "        html = ''\n",
    "        if hasattr(e, 'code'):\n",
    "            code = e.code\n",
    "            if num_retries > 0 and 500 <= code < 600:\n",
    "                # retry 5XX HTTP errors\n",
    "                return download(url, headers, proxy, num_retries-1, data)\n",
    "        else:\n",
    "            code = None\n",
    "    return html\n",
    "\n",
    "\n",
    "def normalize(seed_url, link):\n",
    "    \"\"\"Normalize this URL by removing hash and adding domain\n",
    "    \"\"\"\n",
    "    link, _ = urlparse.urldefrag(link) # remove hash to avoid duplicates\n",
    "    return urlparse.urljoin(seed_url, link)\n",
    "\n",
    "\n",
    "def same_domain(url1, url2):\n",
    "    \"\"\"Return True if both URL's belong to same domain\n",
    "    \"\"\"\n",
    "    return urlparse.urlparse(url1).netloc == urlparse.urlparse(url2).netloc\n",
    "\n",
    "\n",
    "def get_robots(url):\n",
    "    \"\"\"Initialize robots parser for this domain\n",
    "    \"\"\"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(urlparse.urljoin(url, '/robots.txt'))\n",
    "    rp.read()\n",
    "    return rp\n",
    "        \n",
    "\n",
    "def get_links(html):\n",
    "    \"\"\"Return a list of links from html \n",
    "    \"\"\"\n",
    "    # a regular expression to extract all links from the webpage\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    # list of all links from the webpage\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    link_crawler('http://example.webscraping.com', '/(index|view)', delay=0, num_retries=1, user_agent='BadCrawler')\n",
    "    link_crawler('http://example.webscraping.com', '/(index|view)', delay=0, num_retries=1, max_depth=1, user_agent='GoodCrawler')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv1_performance.py2\n",
    "import re\n",
    "def re_scraper(html):\n",
    "\tresults = {}\n",
    "\tfor field in FIELDS:\n",
    "\t\tresults[field] = re.search('<tr id=\"places_%s__row\">.*?<td class=\"w2p_fw\">(.*?)</td>' % field, html).groups()[0]\n",
    "\t\n",
    "\treturn results\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def bs_scraper(html):\n",
    "\tsoup = BeautifulSoup(html,'html.parser')\n",
    "\tresults = {}\n",
    "\tfor field in FIELDS:\n",
    "\t\tresults[field] = soup.find('table').find('tr',id='places_%s__row'%field).find('td',class_='w2p_fw').text\n",
    "\treturn results\n",
    "\n",
    "import lxml.html\n",
    "def lxml_scraper(html):\n",
    "\ttree = lxml.html.fromstring(html)\n",
    "\tresults = {}\n",
    "\tfor field in FIELDS:\n",
    "\t\tresults[field] = tree.cssselect('table>tr#places_%s__row > td.w2p_fw' %field)[0].text_content()\n",
    "\treturn results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readme\n",
    "fv1\n",
    "\tfunction1 #download()\n",
    "\tfunction2 #crawl_sitemap()\n",
    "\tfunction3\n",
    "\tfunction4\n",
    "\trun0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run0.py\n",
    "'''\n",
    "if __name__=='__main__':\n",
    "\timport sys\n",
    "\tsys.path.append('/home/kishore/p/webscrap/')\n",
    "\n",
    "\tfrom webscrap1 import *\n",
    "\tfrom function2 import *\n",
    "\n",
    "\t\n",
    "'''\n",
    "#download_function.py2\n",
    "import urllib2\n",
    "\n",
    "def download(url, user_agent = 'orek', num_retries=2):\n",
    "   print 'Downloading:',url\n",
    "   headers = {'User_agent':user_agent}\n",
    "   request = urllib2.Request(url,headers = headers)\n",
    "   \n",
    "   try:\n",
    "      html = urllib2.urlopen(request).read()\n",
    "\t\n",
    "   except urllib2.URLError as e:\n",
    "      print 'Download error:',e.reason\n",
    "      html = None\n",
    "\t\t\n",
    "      if num_retries > 0:\n",
    "         if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "            #recursively retry 5xx http errors\n",
    "            return download(url, num_retries-1)\n",
    "   return html\n",
    "\n",
    "\n",
    "#crawl_sitemap.py2\n",
    "\n",
    "def crawl_sitemap(url):\n",
    "   import re\n",
    "\t#download the sitemap file\n",
    "   sitemap = download(url)\n",
    "\t#extract the sitemap links\n",
    "   links =  re.findall('<loc>(.*?)</loc>', sitemap)\n",
    "\t#download each link\n",
    "   for link in links:\n",
    "\t\thtml = download(link)\n",
    "\t\t# scrape html here\n",
    "\t\t#...\n",
    "\n",
    "crawl_sitemap('http://example.webscraping.com/sitemap.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapingways.py2\n",
    "#scrapingWays.py2\n",
    "\n",
    "#---scraping with regular expression-----\n",
    "from function1 import *\n",
    "import re\n",
    "\n",
    "url = 'http://example.webscraping.com/view/United-Kingdom-239'\n",
    "html = download(url)\n",
    "\n",
    "re.findall('<td class=\"w2p_fw\">(.*?)</td>',html)\n",
    "#re.findall('<td class=\"w2p_fw\">(.*?)</td>',html) [1] '244,820 square kilometers'\n",
    "\n",
    "#-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webscrap1.py2\n",
    "#! python\n",
    "\n",
    "#download_function.py2\n",
    "import urllib2\n",
    "\n",
    "def download(url, user_agent = 'orek', num_retries=2):\n",
    "   print 'Downloading:',url\n",
    "\theaders = {'User_agent':user_agent}\n",
    "\trequest = urllib2.Request(url,headers = headers)\n",
    "   \n",
    "\ttry:\n",
    "      html = urllib2.urlopen(url).read()\n",
    "\t\n",
    "   except urllib2.URLError as e:\n",
    "      print 'Download error:',e.reason\n",
    "      html = None\n",
    "\t\t\n",
    "      if num_retries > 0:\n",
    "         if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "            #recursively retry 5xx http errors\n",
    "            return download(url, num_retries-1)\n",
    "   return html\n",
    "\n",
    "#test run\n",
    "#download('http://httpstat.us/500')\n",
    "\n",
    "\n",
    "#copy\n",
    "#! python\n",
    "\n",
    "#download_function.py2\n",
    "import urllib2\n",
    "\n",
    "def download(url, user_agent = 'orek', num_retries=2):\n",
    "   print 'Downloading:',url\n",
    "   headers = {'User_agent':user_agent}\n",
    "   request = urllib2.Request(url,headers = headers)\n",
    "   \n",
    "   try:\n",
    "      html = urllib2.urlopen(request).read()\n",
    "\t\n",
    "   except urllib2.URLError as e:\n",
    "      print 'Download error:',e.reason\n",
    "      html = None\n",
    "\t\t\n",
    "      if num_retries > 0:\n",
    "         if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "            #recursively retry 5xx http errors\n",
    "            return download(url, num_retries-1)\n",
    "   return html\n",
    "\n",
    "#test run\n",
    "#download('http://httpstat.us/500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webscrapfunctions.py2\n",
    "#webscrapfunc.py\n",
    "#web scraping functions\n",
    "\n",
    "from function1 import * #download()\n",
    "from function2 import *\t#crawl_sitemap()\n",
    "from function3 import * #id_iter_crawl()\n",
    "from function4 import * #link_crawler()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
